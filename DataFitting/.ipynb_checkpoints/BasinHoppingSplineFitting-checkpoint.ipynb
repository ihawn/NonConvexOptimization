{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"basin_hopping.jl\")\n",
    "using Plots, Polynomials, CubicSplines, LaTeXStrings, Calculus, Optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0700785",
   "metadata": {},
   "source": [
    "# Curve Fitting With Basin Hopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428c774f",
   "metadata": {},
   "source": [
    "Convex problems have a multitude of \"nice\" properties, not the least of which is the assurance that any point where $\\nabla f = 0$ gives us a global optimal solution. This gives rise to certain certificates of optimality which may be used to be sure that a solution $x$ is optimal.\n",
    "\n",
    "Conversely, non-convex problems do not have these nice properties. We have no guarantee that any local solution is the global solution and in fact, there may be infinitely many global solutions.\n",
    "\n",
    "However, many real world problems are non-convex, making finding a means of optimizing them all the more important. For example, consider the following sample data $(\\overline{x}_1, \\overline{y}_1),\\ldots,(\\overline{x}_k, \\overline{y}_k)$ and suppose we want to fit it to the model:\n",
    "\n",
    "$$\n",
    "f(x) = a\\sin(bx) + b\\cos(ax)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc8119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actual parameters\n",
    "a = 4.0\n",
    "b = 6.0\n",
    "\n",
    "f(x) = a*sin(b*x) + b*cos(a*x)\n",
    "\n",
    "#Generate noisy data\n",
    "datasize = 600\n",
    "variation = 6\n",
    "x_range = range(-10, 10, length=datasize)\n",
    "y_range = f.(x_range) + randn(datasize)*variation\n",
    "\n",
    "scatter(x_range, y_range, label = \"Data points\", color=:black, markersize = 2)\n",
    "plot!(f, legend=:topleft, label = \"True Curve\", color=:red, xrange = (0, 10), linewidth = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69179c26",
   "metadata": {},
   "source": [
    "To solve such a problem, we would want minimize the value $L(a, b) = \\|\\mathbf{\\overline{y}} - \\mathbf{y}\\|_2^2$, where vector $\\mathbf{\\overline{y}}$ contains each $\\overline{y}$ and vector $\\mathbf{y} = a\\sin(bx) + b\\cos(ax),\\;x\\in\\{\\overline{x}_1,\\ldots,\\overline{x}_k\\}$. Graphing $L$, we can clearly see that this objective is non-convex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f17a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_params(_a, _b) = [_a*sin(_b*x) + _b*cos(_a*x) for x in x_range]\n",
    "f_params(p) = f_params(p...)\n",
    "Loss(p) = norm(y_range - f_params(p))\n",
    "Loss_plotter(_a, _b) = Loss([_a, _b]);\n",
    "aa = -10:.05:10\n",
    "bb = -10:.05:10\n",
    "\n",
    "contour(aa, bb, Loss_plotter, fill=true, title = \"Loss Function\")\n",
    "plot!(xlabel=L\"a\", ylabel=L\"b\", size=(600,600), legend=:none)\n",
    "scatter!([a], [b], color=:white, label = \"Known Solution\", markersize = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d86bb3",
   "metadata": {},
   "source": [
    "### Basin Hopping Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072b54e4",
   "metadata": {},
   "source": [
    "Basin Hopping is a practical approach for non-convex optimization. Though it lacks any proofs of convergence, this method is one of the most common global optimization techniques used in practice, due to its speed and ease of dimensional generalization.\n",
    "\n",
    "Its premise is simple: start somewhere in the domain of objective $f$ and minimize locally. Then take sample points in the neighborhood of this local solution.\n",
    "\n",
    "We may either take these points as they are or optimize the objective from each and pick the lowest solution. From these collection of points, we select one to be our next starting point. We select based on the following criterion called the Metropolis Acceptance Criterion. For previous solution $x^*$, current solution $\\hat{x}^*$, and random point $\\xi\\in [0,1]$, we pick the next starting point based on the following:\n",
    "\n",
    "$$\n",
    "x^* = \\begin{cases} \\hat{x}^*,\\; f(\\hat{x}^*) < f(x^*) \\text{ or } \\xi \\leq \\exp\\left(\\min\\left(0, \\frac{f(x^*)-f(\\hat{x}^*)}{T}\\right)\\right) \\\\ x^*,\\;\\text{ else} \\end{cases}\n",
    "$$\n",
    "\n",
    "Where $T\\in (0, 1]$ is a parameter. So if $f(\\hat{x}^*) < f(x^*)$, then we accept $\\hat{x}^*$ automatically. Otherwise, we accept it according to the probability curve $\\exp\\left(\\min\\left(0, \\frac{f(x^*)-f(\\hat{x}^*)}{T}\\right)\\right)$. This is visualized below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c300fc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_val = 1.0\n",
    "T = 0.8\n",
    "\n",
    "dd(diff) = exp(min(0, (diff)/T))\n",
    "\n",
    "x_range1 = Float64[]\n",
    "x_range2 = Float64[]\n",
    "ξ_accept = Float64[]\n",
    "ξ_reject = Float64[]\n",
    "for i = 1:500\n",
    "    ξ = rand()\n",
    "    x = 5*rand() - 3\n",
    "    if ξ < dd(x)\n",
    "        append!(ξ_accept, ξ)\n",
    "        append!(x_range1, x)\n",
    "    else\n",
    "        append!(ξ_reject, ξ)\n",
    "        append!(x_range2, x)\n",
    "    end\n",
    "end\n",
    "\n",
    "plot(legend=:topleft, xlims = (-3, 2), xaxis = L\"f(x^*)-f(\\hat{x}^*)\", yaxis = L\"\\xi\")\n",
    "scatter!(x_range1, ξ_accept, color=:green, label = \"Accept\")\n",
    "scatter!(x_range2, ξ_reject, color=:red, label = \"Reject\")\n",
    "plot!(dd, label = \"Probability Curve\", linewidth = 5, color=:blue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f23ad95",
   "metadata": {},
   "source": [
    "We may also adjust the length $\\ell$ of the search neighborhood by tracking the acceptance rate. If the acceptance rate is less then our target, then the algorithm is returning few better solutions so we scale $\\ell$ up to search a larger area. Conversely, if the acceptance rate is too high, than we may be trapped in a basin so scale $\\ell$ down until the target acceptance rate is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521ff7d7",
   "metadata": {},
   "source": [
    "### Problem Formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1f70c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actual parameters\n",
    "a = 4.0\n",
    "b = 6.0\n",
    "\n",
    "f(x) = a*sin(b*x) + b*cos(a*x)\n",
    "\n",
    "#Generate noisy data\n",
    "datasize = 2048\n",
    "variation = 4\n",
    "x_range = range(-10, 10, length=datasize)\n",
    "y_range = f.(x_range) + randn(datasize)*variation\n",
    "\n",
    "scatter(x_range, y_range, label = \"Data points\", color=:black, markersize = 2)\n",
    "plot!(f, legend=:topleft, label = \"True Curve\", color=:red, xrange = (-10, 10), linewidth = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f694a138",
   "metadata": {},
   "source": [
    "So our goal is to find the best fit by minimizing the loss $L(a, b) = \\|\\mathbf{\\overline{y}} - \\mathbf{y}\\|_2^2$. We will try to solve the problem in $2$ ways. In the first way, we will first assume we know what the function structure is. In the second way, we will not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a447b703",
   "metadata": {},
   "source": [
    "### Finding Best Fit When We Know The Problem Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d768af",
   "metadata": {},
   "source": [
    "Firstly, consider the case when we know that our original $f$ is in the form\n",
    "\n",
    "$$\n",
    "f(x) = a\\sin(bx) + b\\cos(ax)\n",
    "$$\n",
    "\n",
    "If this is the case, then we can simply construct our loss function as a function of $a$ and $b$ like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889cbdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_params(_a, _b) = [_a*sin(_b*x) + _b*cos(_a*x) for x in x_range]\n",
    "f_params(p) = f_params(p...)\n",
    "Loss(p) = norm(y_range - f_params(p))\n",
    "Loss_plotter(_a, _b) = Loss([_a, _b]);\n",
    "aa = -10:.05:10\n",
    "bb = -10:.05:10\n",
    "\n",
    "contour(aa, bb, Loss_plotter, fill=true, title = \"Loss Function\")\n",
    "plot!(xlabel=L\"a\", ylabel=L\"b\", size=(600,600), legend=:none)\n",
    "scatter!([a], [b], color=:white, label = \"Known Solution\", markersize = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd20248",
   "metadata": {},
   "source": [
    "We may then perform Basin Hopping to obtain our solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b1bed0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "global now = time()\n",
    "xPlot = []\n",
    "yPlot = []\n",
    "solPlotX = []\n",
    "solPlotY = []\n",
    "searchX = []\n",
    "searchY = []\n",
    "minsX = []\n",
    "minsY = []\n",
    "finalSolX = []\n",
    "finalSolY = []\n",
    "distNoiseX = []\n",
    "distNoiseY = []\n",
    "timings = []\n",
    "normDif = []\n",
    "\n",
    "n = 2\n",
    "rng = 250\n",
    "minX = -rng\n",
    "maxX = rng\n",
    "rand_num_points = 100\n",
    "x0 = rng*(2*rand(n) .- 1)\n",
    "ϵ = 1e-8\n",
    "η = 1e-2\n",
    "α = 0.5\n",
    "β = 0.8\n",
    "κ = 1\n",
    "ℓ = 0.4\n",
    "ℓ_range = (0.2, abs(maxX - minX))\n",
    "γ = 0.9\n",
    "ϕ = 0.0\n",
    "T = 1\n",
    "static_threshold = 10\n",
    "target_acc_rate = 0.6\n",
    "maxIterations = 500\n",
    "\n",
    "runtime = @elapsed minSol = Basin_Hopping(Loss, x0, rand_num_points, minX, maxX, α, \n",
    "    β, η, ϵ, κ, ℓ, ℓ_range, γ, ϕ, T, target_acc_rate, maxIterations, static_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c629da9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "minSol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5515d1",
   "metadata": {},
   "source": [
    "We see that we get very close to our original parameters. Due to the fact that we generated the noisy datapoints randomly, the global minimum of our loss function will not be precisely the solution to our original problem. However, depending on the number of datapoints we have, it should be relatively close.\n",
    "\n",
    "We may see just how close our solution is by plotting it with our new parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ff2ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = minSol[1]\n",
    "sol(x) = params[1]*sin(params[2]*x) + params[2]*cos(params[1]*x)\n",
    "plot(f, label = \"Original f\", title = \"Original Function v. Best Fit\", xrange = (-10, 10), color =:red, markersize = 2)\n",
    "plot!(sol, label = \"Best Fit\", color=:green, markersize = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4f6d88",
   "metadata": {},
   "source": [
    "### Finding Best Fit When We Do Not Know The Problem Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d4cfc1",
   "metadata": {},
   "source": [
    "When we do not know the problem structure, we may approximate pieces of it with a polynomial and join each resulting polynomial with a cubic spline. This has the added benefit of being extremely parallelizable.\n",
    "\n",
    "However, how do we pick our polynomial degree? Basin Hopping problems become increasingly more difficult to solve with more parameters but we do not want to lose accuracy either.\n",
    "\n",
    "Let us test solutions of varying degree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7db51df",
   "metadata": {},
   "source": [
    "#### 0th Degree Polynomial Fitting (Step Function)\n",
    "\n",
    "Firstly, we will attempt to fit the data with a step function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7fc09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_params(_a, _b) = [_a*sin(_b*x) + _b*cos(_a*x) for x in x_range]\n",
    "f_params(p) = f_params(p...)\n",
    "Loss(p) = norm(y_range - f_params(p))\n",
    "Loss_plotter(_a, _b) = Loss([_a, _b]);\n",
    "aa = -10:.05:10\n",
    "bb = -10:.05:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a68a0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "threads = 64\n",
    "k = length(x_range)\n",
    "\n",
    "x_data = [[x_range[n] for n in Int((i-1)*k/threads) + 1:Int(i*k/threads)] for i in 1:threads]\n",
    "y_data = [[y_range[n] for n in Int((i-1)*k/threads) + 1:Int(i*k/threads)] for i in 1:threads];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a05f28c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_funcs = Array{Function}(undef, threads)\n",
    "for i = 1:threads\n",
    "    dat = x_data[i]\n",
    "    dat_y = y_data[i]\n",
    "    if i != threads\n",
    "        dat = vcat(x_data[i], x_data[i+1])\n",
    "        dat_y = vcat(y_data[i], y_data[i+1])\n",
    "    end\n",
    "    loss_funcs[i] = p -> norm(dat_y .- [p[1] for x in dat])\n",
    "end\n",
    "plot(loss_funcs, xrange = (-10, 10), legend = false, title = \"Loss Functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c859c4d",
   "metadata": {},
   "source": [
    "As can be seen, each of our many loss functions can simply be solved using traditional optimization techniques. We do not need Basin Hopping yet. Observe the following solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff191b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "constSols = []\n",
    "for i = 1:threads\n",
    "    x = 1.0\n",
    "    o = optimize(loss_funcs[i], [x])\n",
    "    push!(constSols, Polynomial(Optim.minimizer(o)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bec08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 4.0\n",
    "b = 6.0\n",
    "\n",
    "f(x) = a*sin(b*x) + b*cos(a*x)\n",
    "\n",
    "p = plot(f, yrange = (-20, 20), xrange = (-10, 10), color=:red, title = \"Step Fit: All Threads\")\n",
    "\n",
    "for i = 1:threads\n",
    "    plot!(x_data[i], [constSols[i](x) for x ∈ x_data[i]], color=:green, markersize = 2,xrange = (-10, 10), legend = false)\n",
    "end\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0ca84c",
   "metadata": {},
   "source": [
    "The previous graph depicts the raw step function. Next, we add a cubic spline to make the result continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed85faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spline_x = Float64[]\n",
    "spline_y = Float64[]\n",
    "for i = 1:threads\n",
    "    spline_x = vcat(spline_x, x_data[i])\n",
    "    spline_y = vcat(spline_y, [constSols[i](x) for x ∈ x_data[i]])\n",
    "end\n",
    "\n",
    "spline = CubicSpline(spline_x, spline_y)\n",
    "xs = range(-10, 10, length=2000)\n",
    "ys = spline[xs]\n",
    "\n",
    "p = plot(f, yrange = (-20, 20), xrange = (-10, 10), color=:red, label = \"f\", title = \"Step Fit With Cubic Spline Interpolation\")\n",
    "plot!(xs, ys, color=:green, label = \"Fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdeb338",
   "metadata": {},
   "source": [
    "The result is not very accurate but it does give us a good baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e37058",
   "metadata": {},
   "source": [
    "#### 1st Degree Polynomial Fitting (Linear)\n",
    "\n",
    "Now, we will add a second parameter to our approximation model, turning it into a linear approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52008f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_funcs = Array{Function}(undef, threads)\n",
    "for i = 1:threads\n",
    "    dat = x_data[i]\n",
    "    dat_y = y_data[i]\n",
    "    if i != threads\n",
    "        dat = vcat(x_data[i], x_data[i+1])\n",
    "        dat_y = vcat(y_data[i], y_data[i+1])\n",
    "    end\n",
    "    loss_funcs[i] = p -> norm(dat_y .- [p[2]*x + p[1] for x in dat])\n",
    "end\n",
    "\n",
    "loss_funcs[1]([1.0,2.0]) #testing running one of the functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b589e282",
   "metadata": {},
   "source": [
    "First, we will try a fit using pure Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26de6d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "polySols = []\n",
    "xPlot = []\n",
    "yPlot = []\n",
    "global now = time()\n",
    "solPlotX = []\n",
    "solPlotY = []\n",
    "searchX = []\n",
    "searchY = []\n",
    "minsX = []\n",
    "minsY = []\n",
    "finalSolX = []\n",
    "finalSolY = []\n",
    "distNoiseX = []\n",
    "distNoiseY = []\n",
    "timings = []\n",
    "normDif = []\n",
    "for i = 1:threads\n",
    "    n = 2\n",
    "    rng = 50\n",
    "    x0 = rng*(2*rand(n) .- 1)\n",
    "    η = 1e-4\n",
    "    α = 0.5\n",
    "    β = 0.8\n",
    "    κ = 1\n",
    "    x, fx = Grad_Descent(loss_funcs[i], x0, α, β, η, κ, 500)\n",
    "    push!(polySols, Polynomial(x))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3905257",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 4.0\n",
    "b = 6.0\n",
    "\n",
    "f(x) = a*sin(b*x) + b*cos(a*x)\n",
    "\n",
    "p = plot(f, yrange = (-20, 20), xrange = (-10, 10), color=:red, title = \"Linear Fit: Pure Gradient Descent\")\n",
    "\n",
    "for i = 1:threads\n",
    "    plot!(x_data[i], [polySols[i](x) for x ∈ x_data[i]], color=:green, markersize = 2,xrange = (-10, 10), legend = false)\n",
    "end\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e48437d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spline_x = Float64[]\n",
    "spline_y = Float64[]\n",
    "for i = 1:threads\n",
    "    spline_x = vcat(spline_x, x_data[i])\n",
    "    spline_y = vcat(spline_y, [polySols[i](x) for x ∈ x_data[i]])\n",
    "end\n",
    "\n",
    "spline = CubicSpline(spline_x, spline_y)\n",
    "xs = range(-10, 10, length=1000)\n",
    "ys = spline[xs]\n",
    "\n",
    "p = plot(f, yrange = (-20, 20), xrange = (-10, 10), color=:red, label = \"f\", title = \"Gradient Descent Linear Fit With Cubic Splines\")\n",
    "plot!(xs, ys, color=:green, label = \"Fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fe8c59",
   "metadata": {},
   "source": [
    "As can be seen, this fit is not very accurate since pure Gradient Descent fails to find the global minimum of our loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18a35a9",
   "metadata": {},
   "source": [
    "We can improve on this fit significantly by using basin hopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e9954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "polySols = []\n",
    "for i = 1:threads\n",
    "    print(\"Thread: \", i)\n",
    "    global now = time()\n",
    "    xPlot = []\n",
    "    yPlot = []\n",
    "    solPlotX = []\n",
    "    solPlotY = []\n",
    "    searchX = []\n",
    "    searchY = []\n",
    "    minsX = []\n",
    "    minsY = []\n",
    "    finalSolX = []\n",
    "    finalSolY = []\n",
    "    distNoiseX = []\n",
    "    distNoiseY = []\n",
    "    timings = []\n",
    "    normDif = []\n",
    "\n",
    "    n = 2\n",
    "    rng = 250\n",
    "    minX = -rng\n",
    "    maxX = rng\n",
    "    rand_num_points = 5\n",
    "    x0 = rng*(2*rand(n) .- 1)\n",
    "    ϵ = 1e-8\n",
    "    η = 1e-2\n",
    "    α = 0.5\n",
    "    β = 0.8\n",
    "    κ = 1\n",
    "    ℓ = 0.4\n",
    "    ℓ_range = (0.2, abs(maxX - minX))\n",
    "    γ = 0.9\n",
    "    ϕ = 0.0\n",
    "    T = 1\n",
    "    static_threshold = 3\n",
    "    target_acc_rate = 0.6\n",
    "    maxIterations = 15\n",
    "    \n",
    "    runtime = @elapsed minSol = Basin_Hopping(loss_funcs[i], x0, rand_num_points, minX, maxX, α, \n",
    "        β, η, ϵ, κ, ℓ, ℓ_range, γ, ϕ, T, target_acc_rate, maxIterations, static_threshold)\n",
    "    push!(polySols, Polynomial(minSol[1]))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21351f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 4.0\n",
    "b = 6.0\n",
    "\n",
    "f(x) = a*sin(b*x) + b*cos(a*x)\n",
    "\n",
    "p = plot(f, yrange = (-20, 20), xrange = (-10, 10), color=:red, title = \"Linear Fit: Basin Hopping\")\n",
    "\n",
    "for i = 1:threads\n",
    "    plot!(x_data[i], [polySols[i](x) for x ∈ x_data[i]], color=:green, markersize = 2,xrange = (-10, 10), legend = false)\n",
    "end\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a569da31",
   "metadata": {},
   "outputs": [],
   "source": [
    "spline_x = Float64[]\n",
    "spline_y = Float64[]\n",
    "for i = 1:threads\n",
    "    spline_x = vcat(spline_x, x_data[i])\n",
    "    spline_y = vcat(spline_y, [polySols[i](x) for x ∈ x_data[i]])\n",
    "end\n",
    "\n",
    "spline = CubicSpline(spline_x, spline_y)\n",
    "xs = range(-10, 10, length=1000)\n",
    "ys = spline[xs]\n",
    "\n",
    "p = plot(f, yrange = (-20, 20), xrange = (-10, 10), color=:red, label = \"f\", title = \"Basin Hopping Linear Fit With Cubic Splines\")\n",
    "plot!(xs, ys, color=:green, label = \"Fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0682055",
   "metadata": {},
   "source": [
    "We observe a much better fit after using Basin Hopping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c609c868",
   "metadata": {},
   "source": [
    "#### 2nd Degree Polynomial Fitting\n",
    "\n",
    "We will now try to fit our data with a quadratic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349cb7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_funcs = Array{Function}(undef, threads)\n",
    "for i = 1:threads\n",
    "    dat = x_data[i]\n",
    "    dat_y = y_data[i]\n",
    "    if i != threads\n",
    "        dat = vcat(x_data[i], x_data[i+1])\n",
    "        dat_y = vcat(y_data[i], y_data[i+1])\n",
    "    end\n",
    "    loss_funcs[i] = p -> norm(dat_y .- [p[3]*x^2 + p[2]*x + p[1] for x in dat])\n",
    "end\n",
    "\n",
    "loss_funcs[1]([1.0,2.0, 3.0]) #testing running one of the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cb3561",
   "metadata": {},
   "outputs": [],
   "source": [
    "polySols = []\n",
    "for i = 1:threads\n",
    "    global now = time()\n",
    "    xPlot = []\n",
    "    yPlot = []\n",
    "    solPlotX = []\n",
    "    solPlotY = []\n",
    "    searchX = []\n",
    "    searchY = []\n",
    "    minsX = []\n",
    "    minsY = []\n",
    "    finalSolX = []\n",
    "    finalSolY = []\n",
    "    distNoiseX = []\n",
    "    distNoiseY = []\n",
    "    timings = []\n",
    "    normDif = []\n",
    "\n",
    "    n = 3\n",
    "    rng = 200.0\n",
    "    minX = -rng\n",
    "    maxX = rng\n",
    "    rand_num_points = 5\n",
    "    x0 = rng*(2*rand(n) .- 1)\n",
    "    ϵ = 1e-8\n",
    "    η = 1e-2\n",
    "    α = 0.5\n",
    "    β = 0.8\n",
    "    κ = 1\n",
    "    ℓ = 0.4\n",
    "    ℓ_range = (0.2, abs(maxX - minX))\n",
    "    γ = 0.9\n",
    "    ϕ = 0.0\n",
    "    T = 1\n",
    "    static_threshold = 5\n",
    "    target_acc_rate = 0.6\n",
    "    maxIterations = 50\n",
    "    \n",
    "    runtime = @elapsed minSol = Basin_Hopping(loss_funcs[i], x0, rand_num_points, minX, maxX, α, \n",
    "        β, η, ϵ, κ, ℓ, ℓ_range, γ, ϕ, T, target_acc_rate, maxIterations, static_threshold)\n",
    "    push!(polySols, Polynomial(minSol[1]))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34284917",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 4.0\n",
    "b = 6.0\n",
    "\n",
    "f(x) = a*sin(b*x) + b*cos(a*x)\n",
    "\n",
    "p = plot(f, yrange = (-20, 20), xrange = (-10, 10), color=:red, title = \"Quadratic Fit: All Threads\")\n",
    "\n",
    "for i = 1:threads\n",
    "    plot!(x_data[i], [polySols[i](x) for x ∈ x_data[i]], color=:green, markersize = 2,xrange = (-10, 10), legend = false)\n",
    "end\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8bc194",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spline_x = Float64[]\n",
    "spline_y = Float64[]\n",
    "for i = 1:threads\n",
    "    spline_x = vcat(spline_x, x_data[i])\n",
    "    spline_y = vcat(spline_y, [polySols[i](x) for x ∈ x_data[i]])\n",
    "end\n",
    "\n",
    "spline = CubicSpline(spline_x, spline_y)\n",
    "xs = range(-10, 10, length=2000)\n",
    "ys = spline[xs]\n",
    "\n",
    "p = plot(f, yrange = (-20, 20), xrange = (-10, 10), color=:red, label = \"f\", title = \"Quadratic Fit With Cubic Splines\")\n",
    "plot!(xs, ys, color=:green, label = \"Fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e322a1",
   "metadata": {},
   "source": [
    "We observe a good fit. However, adding the third parameter caused this computation to take significantly longer than the linear fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc935065",
   "metadata": {},
   "source": [
    "### Solving More Difficult Fits\n",
    "\n",
    "Performing the piecewise fit as we have been doing has other benefits as well. Suppose our function $f:\\mathbb{R}\\to\\mathbb{R}$ had many more parameters than $2$. Even if we knew the strucure of $f$, it might not be practical to solve it by fitting the parameters exactly. \n",
    "\n",
    "However, by using the piecewise fit technique, we may produce good results in reasonable time. As an example, consider the following generalization of our function from before:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n \\left(a_i\\sin(b_i x) + b_i\\cos(a_i x)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f30de44",
   "metadata": {},
   "source": [
    "Here, using the piecewise fit method has a significant advantage in that the number of parameters here is essentially irrelevent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0c7d95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nn = 10\n",
    "a = 5*randn(n)\n",
    "b = 5*randn(n)\n",
    "fff(x) = sum(a[i]*sin(b[i]*x) + b[i]*cos(a[i]*x) for i ∈ 1:nn)\n",
    "\n",
    "#Generate noisy data\n",
    "datasize = 8192\n",
    "variation = 10\n",
    "x_range = range(-10, 10, length=datasize)\n",
    "y_range = fff.(x_range) + randn(datasize)*variation\n",
    "\n",
    "scatter(x_range, y_range, label = \"Data points\", color=:black, markersize = 1)\n",
    "plot!(fff, legend=:topleft, label = \"True Curve\", color=:red, xrange = (-10, 10), linewidth = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71411cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = 128\n",
    "k = length(x_range)\n",
    "\n",
    "x_data = [[x_range[n] for n in Int((i-1)*k/threads) + 1:Int(i*k/threads)] for i in 1:threads]\n",
    "y_data = [[y_range[n] for n in Int((i-1)*k/threads) + 1:Int(i*k/threads)] for i in 1:threads];\n",
    "\n",
    "loss_funcs = Array{Function}(undef, threads)\n",
    "for i = 1:threads\n",
    "    dat = x_data[i]\n",
    "    dat_y = y_data[i]\n",
    "    if i != threads\n",
    "        dat = vcat(x_data[i], x_data[i+1])\n",
    "        dat_y = vcat(y_data[i], y_data[i+1])\n",
    "    end\n",
    "    loss_funcs[i] = p -> norm(dat_y .- [p[2]*x + p[1] for x in dat])\n",
    "end\n",
    "\n",
    "loss_funcs[1]([1.0,2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b5bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "polySols = []\n",
    "for i = 1:threads\n",
    "    print(\"Thread: \", i)\n",
    "    global now = time()\n",
    "    xPlot = []\n",
    "    yPlot = []\n",
    "    solPlotX = []\n",
    "    solPlotY = []\n",
    "    searchX = []\n",
    "    searchY = []\n",
    "    minsX = []\n",
    "    minsY = []\n",
    "    finalSolX = []\n",
    "    finalSolY = []\n",
    "    distNoiseX = []\n",
    "    distNoiseY = []\n",
    "    timings = []\n",
    "    normDif = []\n",
    "\n",
    "    n = 2\n",
    "    rng = 1000\n",
    "    minX = -rng\n",
    "    maxX = rng\n",
    "    rand_num_points = 25\n",
    "    x0 = rng*(2*rand(n) .- 1)\n",
    "    ϵ = 1e-8\n",
    "    η = 1e-2\n",
    "    α = 0.5\n",
    "    β = 0.8\n",
    "    κ = 1\n",
    "    ℓ = 0.4\n",
    "    ℓ_range = (0.2, abs(maxX - minX))\n",
    "    γ = 0.9\n",
    "    ϕ = 0.0\n",
    "    T = 1\n",
    "    static_threshold = 10\n",
    "    target_acc_rate = 0.6\n",
    "    maxIterations = 100\n",
    "    \n",
    "    runtime = @elapsed minSol = Basin_Hopping(loss_funcs[i], x0, rand_num_points, minX, maxX, α, \n",
    "        β, η, ϵ, κ, ℓ, ℓ_range, γ, ϕ, T, target_acc_rate, maxIterations, static_threshold)\n",
    "    push!(polySols, Polynomial(minSol[1]))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c750614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plot(fff, yrange = (-50, 50), xrange = (-10, 10), color=:red, title = \"Linear Fit: Basin Hopping\")\n",
    "\n",
    "for i = 1:threads\n",
    "    plot!(x_data[i], [polySols[i](x) for x ∈ x_data[i]], color=:green, markersize = 2,xrange = (-10, 10), yrange = (-50, 50), legend = false)\n",
    "end\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70979153",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spline_x = Float64[]\n",
    "spline_y = Float64[]\n",
    "for i = 1:threads\n",
    "    spline_x = vcat(spline_x, x_data[i])\n",
    "    spline_y = vcat(spline_y, [polySols[i](x) for x ∈ x_data[i]])\n",
    "end\n",
    "\n",
    "spline = CubicSpline(spline_x, spline_y)\n",
    "xs = range(-10, 10, length=2000)\n",
    "ys = spline[xs]\n",
    "\n",
    "p = plot(f, yrange = (-20, 20), xrange = (-10, 10), color=:red, label = \"f\", title = \"Linear Fit With Cubic Splines\")\n",
    "plot!(xs, ys, color=:green, label = \"Fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9cce2e",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "Dr. Krislock\n",
    "\n",
    "M. Iwamatsu and Y. Okabe, Basin hopping with occasional jumping, Chemical\n",
    "Physics Letters, 399 (2004), pp. 396–400.\n",
    "\n",
    "Z. Li and H. A. Scheraga, Monte Carlo-minimization approach to the multiple-\n",
    "minima problem in protein folding, Proceedings of the National Academy of Sciences,\n",
    "84 (1987), pp. 6611–6615.\n",
    "\n",
    "D. J. Wales and J. P. K. Doye, Global optimization by basin-hopping and the lowest\n",
    "energy structures of Lennard-Jones clusters containing up to 110 atoms, The Journal of\n",
    "Physical Chemistry A, 101 (1997), pp. 5111–5116."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
